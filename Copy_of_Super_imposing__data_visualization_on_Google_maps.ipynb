{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Click here to opern this notebook in Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tsekatm/aws-python-data-engineering-challenge/blob/main/Super_imposing__data_visualization_on_Google_maps.ipynb)\n",
        "\n",
        "#üåç AWS Data Engineering Challenge\n",
        "## Superimposing Analysed Loadshedding Data on Google Maps\n",
        "\n",
        "**Host**: Tebogo Tseka\n",
        "\n",
        "**Presenter**: Joyous Konyana\n",
        "\n",
        "üìÖ **Date:**: 24 July 2025\n",
        "\n",
        "**Time**: 19:00 SAST\n",
        "\n",
        "**Zoom Link**: [bit.ly/3VmV3CK](https://bit.ly/3VmV3CK)\n",
        "\n",
        "**Meetup Link / Register here**: [meetup.com/mzansi-aws](https://meetup.com/mzansi-aws)\n",
        "\n",
        "\n",
        "üîó **GitHub:** [your GitHub link]\n",
        "\n",
        "---\n",
        "\n",
        "üéØ **Objective**:\n",
        "Visualize and superimpose real-world, analysed data points on Google Maps using Python.\n",
        "\n",
        "This session demonstrates the power of geospatial analysis and interactive visual storytelling for AWS Data Engineering workloads.\n",
        "\n",
        "üöÄ **Features**\n",
        "\n",
        "\n",
        "- Geospatial data analysis with pandas and folium\n",
        "- Interactive visual maps with real coordinates\n",
        "- Python data manipulation for mapping overlays\n",
        "- Contextual population-based visualizations\n",
        "- Ready for integration into AWS analytics pipelines"
      ],
      "metadata": {
        "id": "lLdgHsW1XPBW"
      },
      "id": "lLdgHsW1XPBW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Install required packages with version constraints for stability"
      ],
      "metadata": {
        "id": "Rp9Xwx72ag8r"
      },
      "id": "Rp9Xwx72ag8r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297bb52c",
      "metadata": {
        "id": "297bb52c"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages with version constraints for stability\"\"\"\n",
        "    packages = [\n",
        "        'boto3>=1.26.0',\n",
        "        'pandas>=1.3.0',\n",
        "        'numpy>=1.21.0',\n",
        "        'matplotlib>=3.3.0',\n",
        "        'seaborn>=0.11.0',\n",
        "        'plotly>=5.0.0',\n",
        "        'scikit-learn>=1.0.0',\n",
        "        'tqdm',  # For progress bars\n",
        "        'psutil'  # For system monitoring\n",
        "    ]\n",
        "\n",
        "    print(\"üì¶ Installing required packages...\")\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
        "            print(f\"‚úÖ {package.split('>=')[0]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
        "\n",
        "    print(\"\\nüéâ Package installation complete!\")\n",
        "\n",
        "install_packages()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError, NoCredentialsError\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from io import BytesIO\n",
        "import joblib\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Configure plotting for Colab\n",
        "pio.renderers.default = \"colab\"\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All packages imported successfully!\")\n",
        "print(\"üöÄ Ready to start our data engineering journey!\")"
      ],
      "metadata": {
        "id": "IpwrJuQkvMw6"
      },
      "id": "IpwrJuQkvMw6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a5926c8b",
      "metadata": {
        "id": "a5926c8b"
      },
      "source": [
        "##Step 2: Setup AWS credentials for Google Colab environment\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_aws_credentials():\n",
        "    \"\"\"Setup AWS credentials for Google Colab environment\"\"\"\n",
        "    print(\"üîê Setting up AWS credentials for Google Colab...\")\n",
        "\n",
        "    try:\n",
        "        # Option 1: Try using Colab secrets (recommended)\n",
        "        from google.colab import userdata\n",
        "        os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "        os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "        print(\"‚úÖ Using Colab secrets for AWS credentials\")\n",
        "        print(\"üí° This is the most secure method!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ÑπÔ∏è Colab secrets not configured, using manual input...\")\n",
        "\n",
        "        # Option 2: Manual input (fallback)\n",
        "        from getpass import getpass\n",
        "        print(\"\\nüîí Please enter your AWS credentials:\")\n",
        "        print(\"   (These will not be displayed or stored)\")\n",
        "\n",
        "        access_key = getpass('Enter AWS Access Key ID: ')\n",
        "        secret_key = getpass('Enter AWS Secret Access Key: ')\n",
        "\n",
        "        if access_key and secret_key:\n",
        "            os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
        "            os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
        "            print(\"‚úÖ AWS credentials configured successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Invalid credentials provided\")\n",
        "            return False\n",
        "\n",
        "# Setup credentials\n",
        "if setup_aws_credentials():\n",
        "    print(\"\\nüìö Learning checkpoint: AWS credentials configured!\")\n",
        "    print(\"üí° You can now proceed with cloud operations.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Please configure AWS credentials before proceeding\")"
      ],
      "metadata": {
        "id": "In38rpNxBtDf"
      },
      "id": "In38rpNxBtDf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Create the AWS Configuration Class\n",
        "\n",
        "This class demonstrates best practices for:\n",
        "\n",
        "    - Cloud service initialization\n",
        "\n",
        "    - Error handling and user feedback\n",
        "\n",
        "    - Resource management in constrained environments"
      ],
      "metadata": {
        "id": "pJ4-je-1bPFd"
      },
      "id": "pJ4-je-1bPFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae80f2d",
      "metadata": {
        "id": "2ae80f2d"
      },
      "outputs": [],
      "source": [
        "class AWSConfig:\n",
        "    \"\"\"\n",
        "    Educational AWS Configuration Manager - Colab Optimized\n",
        "\n",
        "    This class demonstrates best practices for:\n",
        "    - Cloud service initialization\n",
        "    - Error handling and user feedback\n",
        "    - Resource management in constrained environments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # üåç AWS Region selection - affects latency and compliance\n",
        "        self.region = 'us-east-1'  # Northern Virginia - often cheapest for learning\n",
        "\n",
        "        # üì¶ Unique bucket name (S3 bucket names are globally unique)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "        process_id = os.getpid()\n",
        "        self.bucket_name = f'sa-loadshedding-{timestamp}-{process_id}'\n",
        "\n",
        "        try:\n",
        "            # üîó Initialize S3 client with error handling\n",
        "            self.s3_client = boto3.client('s3', region_name=self.region)\n",
        "\n",
        "            # ‚úÖ Test connection by listing buckets\n",
        "            self.s3_client.list_buckets()\n",
        "            print(f\"‚úÖ AWS S3 connected successfully!\")\n",
        "            print(f\"üìç Region: {self.region}\")\n",
        "            print(f\"üì¶ Target bucket: {self.bucket_name}\")\n",
        "\n",
        "        except NoCredentialsError:\n",
        "            print(\"‚ùå AWS credentials not configured!\")\n",
        "            print(\"üîß Please run the credentials setup cell first\")\n",
        "            raise\n",
        "\n",
        "        except ClientError as e:\n",
        "            if e.response['Error']['Code'] == 'InvalidAccessKeyId':\n",
        "                 print(\"‚ùå AWS connection error: Invalid Access Key ID.\")\n",
        "                 print(\"üí° Tip: Please ensure your AWS Access Key ID is correct.\")\n",
        "            elif e.response['Error']['Code'] == 'SignatureDoesNotMatch':\n",
        "                 print(\"‚ùå AWS connection error: Signature Does Not Match.\")\n",
        "                 print(\"üí° Tip: Please ensure your AWS Secret Access Key is correct.\")\n",
        "            else:\n",
        "                print(f\"‚ùå AWS connection error: {e}\")\n",
        "                print(\"üí° Tip: Check your internet connection and AWS credentials\")\n",
        "            raise\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An unexpected AWS connection error occurred: {e}\")\n",
        "            print(\"üí° Tip: Check your internet connection and AWS credentials\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def create_bucket(self):\n",
        "        \"\"\"Creates S3 bucket with proper error handling\"\"\"\n",
        "        try:\n",
        "            # üîç Check if bucket already exists\n",
        "            self.s3_client.head_bucket(Bucket=self.bucket_name)\n",
        "            print(f\"‚úÖ Bucket already exists: {self.bucket_name}\")\n",
        "\n",
        "        except ClientError as e:\n",
        "            error_code = int(e.response['Error']['Code'])\n",
        "\n",
        "            if error_code == 404:\n",
        "                # üèóÔ∏è Bucket doesn't exist, create it\n",
        "                try:\n",
        "                    if self.region == 'us-east-1':\n",
        "                        # us-east-1 doesn't need LocationConstraint\n",
        "                        self.s3_client.create_bucket(Bucket=self.bucket_name)\n",
        "                    else:\n",
        "                        # Other regions need LocationConstraint\n",
        "                        self.s3_client.create_bucket(\n",
        "                            Bucket=self.bucket_name,\n",
        "                            CreateBucketConfiguration={'LocationConstraint': self.region}\n",
        "                        )\n",
        "\n",
        "                    print(f\"‚úÖ Created new bucket: {self.bucket_name}\")\n",
        "                    print(f\"üåç Location: {self.region}\")\n",
        "\n",
        "                except Exception as create_error:\n",
        "                    print(f\"‚ùå Failed to create bucket: {create_error}\")\n",
        "                    print(\"üí° Tip: Bucket names must be globally unique\")\n",
        "                    raise\n",
        "            else:\n",
        "                print(f\"‚ùå Bucket access error: {e}\")\n",
        "                raise\n",
        "\n",
        "# üöÄ Initialize AWS configuration\n",
        "print(\"üîß Setting up AWS infrastructure...\")\n",
        "aws = AWSConfig()\n",
        "aws.create_bucket()\n",
        "\n",
        "print(\"\\nüìö Learning checkpoint: AWS S3 is now ready for our data pipeline!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bb14a6",
      "metadata": {
        "id": "05bb14a6"
      },
      "source": [
        "##Step 4: Optimize DataFrame memory usage for Colab environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nSRw-enJ5MBs",
      "metadata": {
        "id": "nSRw-enJ5MBs"
      },
      "outputs": [],
      "source": [
        "def optimize_memory_usage(df):\n",
        "    \"\"\"Optimize DataFrame memory usage for Colab environment\"\"\"\n",
        "    print(\"üß† Optimizing memory usage...\")\n",
        "\n",
        "    # Store original memory usage\n",
        "    original_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "    # Optimize integer columns\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "\n",
        "    # Optimize float columns\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "\n",
        "    # Calculate memory savings\n",
        "    new_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    savings = original_memory - new_memory\n",
        "\n",
        "    print(f\"üíæ Memory optimized: {original_memory:.1f}MB ‚Üí {new_memory:.1f}MB\")\n",
        "    print(f\"‚úÖ Saved: {savings:.1f}MB ({savings/original_memory*100:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def monitor_system_resources():\n",
        "    \"\"\"Monitor system resources in Colab\"\"\"\n",
        "    # Memory usage\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"üíæ Memory: {memory.used/1024**3:.1f}GB / {memory.total/1024**3:.1f}GB ({memory.percent:.1f}%)\")\n",
        "\n",
        "    # Disk usage\n",
        "    disk = psutil.disk_usage('/')\n",
        "    print(f\"üíø Disk: {disk.used/1024**3:.1f}GB / {disk.total/1024**3:.1f}GB ({disk.used/disk.total*100:.1f}%)\")\n",
        "\n",
        "    if memory.percent > 80:\n",
        "        print(\"‚ö†Ô∏è High memory usage detected!\")\n",
        "        print(\"üí° Consider restarting runtime if performance degrades\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Generate realistic load shedding data for South Africa\n",
        "\n",
        "**This function demonstrates:**\n",
        "\n",
        "\n",
        "    - Time series data generation\n",
        "    - Incorporating domain knowledge into synthetic data\n",
        "    - Seasonal and daily pattern modeling\n",
        "    - Statistical distribution selection\n",
        "\n",
        "**Args:**\n",
        "\n",
        "\n",
        "        start_date (str): Start date for data generation\n",
        "        end_date (str): End date for data generation\n",
        "        freq (str): Frequency of observations ('6H' = every 6 hours)\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "\n",
        "        pd.DataFrame: Generated load shedding data"
      ],
      "metadata": {
        "id": "5yHnPt6pdlCg"
      },
      "id": "5yHnPt6pdlCg"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_loadshedding_data(start_date='2018-01-01', end_date='2024-12-31', freq='6H'):\n",
        "    \"\"\"\n",
        "    Generate realistic load shedding data for South Africa\n",
        "\n",
        "    This function demonstrates:\n",
        "    - Time series data generation\n",
        "    - Incorporating domain knowledge into synthetic data\n",
        "    - Seasonal and daily pattern modeling\n",
        "    - Statistical distribution selection\n",
        "\n",
        "    Args:\n",
        "        start_date (str): Start date for data generation\n",
        "        end_date (str): End date for data generation\n",
        "        freq (str): Frequency of observations ('6H' = every 6 hours)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated load shedding data\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîß Generating realistic load shedding data...\")\n",
        "    print(f\"üìÖ Period: {start_date} to {end_date}\")\n",
        "    print(f\"‚è∞ Frequency: Every {freq}\")\n",
        "\n",
        "    # üóìÔ∏è Create date range\n",
        "    np.random.seed(42)  # For reproducible results\n",
        "    dates = pd.date_range(start_date, end_date, freq=freq)\n",
        "\n",
        "    print(f\"üìä Total time points: {len(dates):,}\")\n",
        "\n",
        "    # üé≤ Base probability distribution for load shedding stages\n",
        "    # Based on South African historical patterns (Stage 0 most common)\n",
        "    base_probabilities = [0.70, 0.15, 0.08, 0.05, 0.02]  # Stages 0-4\n",
        "\n",
        "    # üìä Initialize DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'timestamp': dates,\n",
        "        'stage': np.random.choice([0, 1, 2, 3, 4], len(dates), p=base_probabilities)\n",
        "    })\n",
        "\n",
        "    # üå°Ô∏è Add seasonal effects (Winter = June, July, August in Southern Hemisphere)\n",
        "    print(\"‚ùÑÔ∏è Adding seasonal patterns...\")\n",
        "    data['month'] = data['timestamp'].dt.month\n",
        "    data['is_winter'] = data['month'].isin([6, 7, 8])\n",
        "\n",
        "# Increase load shedding probability in winter\n",
        "    winter_mask = data['is_winter']\n",
        "    winter_increase = np.random.binomial(1, 0.4, winter_mask.sum())  # 40% chance of stage increase\n",
        "    data.loc[winter_mask, 'stage'] = np.minimum(\n",
        "        data.loc[winter_mask, 'stage'] + winter_increase, 4\n",
        "    )\n",
        "    # ‚è∞ Add daily peak hour effects\n",
        "    print(\"üåÖ Adding peak hour patterns...\")\n",
        "    data['hour'] = data['timestamp'].dt.hour\n",
        "    # Peak hours: Morning (6-9) and Evening (17-20)\n",
        "    morning_peak = data['hour'].isin([6, 12, 18])  # 6AM, 12PM, 6PM (6-hour intervals)\n",
        "    peak_increase = np.random.binomial(1, 0.25, morning_peak.sum())  # 25% chance of increase\n",
        "    data.loc[morning_peak, 'stage'] = np.minimum(\n",
        "        data.loc[morning_peak, 'stage'] + peak_increase, 4\n",
        "    )\n",
        "    # üìà Add yearly trend (crisis worsening over time)\n",
        "    print(\"üìà Adding temporal trends...\")\n",
        "    data['year'] = data['timestamp'].dt.year\n",
        "\n",
        "     # Gradual worsening: 2018-2020 (moderate), 2021-2024 (severe)\n",
        "    for year in data['year'].unique():\n",
        "        if year >= 2020:  # Crisis escalation period\n",
        "            year_mask = data['year'] == year\n",
        "            escalation_factor = min((year - 2018) * 0.1, 0.3)  # Max 30% increase\n",
        "            year_increase = np.random.binomial(1, escalation_factor, year_mask.sum())\n",
        "            data.loc[year_mask, 'stage'] = np.minimum(\n",
        "                data.loc[year_mask, 'stage'] + year_increase, 4\n",
        "            )\n",
        "\n",
        "            # üéØ Add economic factors (simplified)\n",
        "    print(\"üíº Adding economic factors...\")\n",
        "    # Higher load shedding during economic stress periods\n",
        "    economic_stress_periods = [\n",
        "        ('2020-03-01', '2020-06-30'),  # COVID-19 impact\n",
        "        ('2022-01-01', '2022-12-31'),  # Energy crisis peak\n",
        "    ]\n",
        "\n",
        "    for start, end in economic_stress_periods:\n",
        "        stress_mask = (data['timestamp'] >= start) & (data['timestamp'] <= end)\n",
        "        stress_increase = np.random.binomial(1, 0.35, stress_mask.sum())\n",
        "        data.loc[stress_mask, 'stage'] = np.minimum(\n",
        "            data.loc[stress_mask, 'stage'] + stress_increase, 4\n",
        "        )\n",
        "\n",
        "        # üßπ Clean up temporary columns\n",
        "    data = data[['timestamp', 'stage']].copy()\n",
        "\n",
        "    # üìä Generate summary statistics\n",
        "    stage_counts = data['stage'].value_counts().sort_index()\n",
        "    total_events = len(data)\n",
        "    active_events = (data['stage'] > 0).sum()\n",
        "\n",
        "    print(\"\\nüìä Generated Data Summary:\")\n",
        "    print(f\"   üìÖ Time range: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
        "    print(f\"   üìà Total observations: {total_events:,}\")\n",
        "    print(f\"   ‚ö° Active load shedding: {active_events:,} ({100*active_events/total_events:.1f}%)\")\n",
        "    print(f\"   üìä Stage distribution:\")\n",
        "    for stage, count in stage_counts.items():\n",
        "        percentage = 100 * count / total_events\n",
        "        print(f\"      Stage {stage}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "    return data\n",
        "\n",
        "    # üöÄ Generate our dataset\n",
        "print(\"üîã Creating South African Load Shedding Dataset...\")\n",
        "loadshedding_data = generate_loadshedding_data()\n",
        "\n",
        "# üß† Optimize memory usage\n",
        "loadshedding_data = optimize_memory_usage(loadshedding_data)\n",
        "\n",
        "print(\"\\nüìö Learning checkpoint: Realistic load shedding data generated!\")\n",
        "print(\"üí≠ Notice how we incorporated domain knowledge into our data generation process.\")\n",
        "\n",
        "# üéØ Progress checkpoint\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ PROGRESS CHECKPOINT - Data Generation Complete\")\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ Dataset size: {loadshedding_data.shape}\")\n",
        "print(f\"üíæ Memory usage: {loadshedding_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "monitor_system_resources()\n",
        "print(\"\\nüéØ Next: Upload to AWS S3\")\n",
        "print(\"=\"*50)\n",
        "\n"
      ],
      "metadata": {
        "id": "FNz3CM0OEHHj"
      },
      "id": "FNz3CM0OEHHj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Augment the existing loadshedding_data with geographical information (Province)\n",
        "- Assuming loadshedding_data DataFrame is already loaded and contains 'timestamp' and 'stage'"
      ],
      "metadata": {
        "id": "lOQ32swaeMWr"
      },
      "id": "lOQ32swaeMWr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad6b6f52",
      "metadata": {
        "id": "ad6b6f52"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Augment the existing loadshedding_data with geographical information (Province)\n",
        "# Assuming loadshedding_data DataFrame is already loaded and contains 'timestamp' and 'stage'\n",
        "\n",
        "# List of South African Provinces\n",
        "provinces = [\n",
        "    'Eastern Cape', 'Free State', 'Gauteng', 'KwaZulu-Natal',\n",
        "    'Limpopo', 'Mpumalanga', 'North West', 'Northern Cape', 'Western Cape'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign a random province to each loadshedding event\n",
        "np.random.seed(42) # for reproducibility\n",
        "loadshedding_data['province'] = np.random.choice(provinces, len(loadshedding_data))"
      ],
      "metadata": {
        "id": "gbi5N4mmyq5q"
      },
      "id": "gbi5N4mmyq5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Augmented loadshedding data with geographical information (Province):\")\n",
        "# Display the first few rows with the new 'province' column\n",
        "display(loadshedding_data.head())"
      ],
      "metadata": {
        "id": "Sj13WkaFyriT"
      },
      "id": "Sj13WkaFyriT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the columns of the DataFrame\n",
        "print(\"\\nDataFrame columns:\")\n",
        "print(loadshedding_data.columns)"
      ],
      "metadata": {
        "id": "97cH64eiyruj"
      },
      "id": "97cH64eiyruj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4dc96c7a",
      "metadata": {
        "id": "4dc96c7a"
      },
      "source": [
        "##Step 7: Geocoding (if necessary)\n",
        "\n",
        "Convert the province names in the `loadshedding_data` DataFrame into geographical coordinates (latitude and longitude) for mapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DXqTmgdE-dXi",
      "metadata": {
        "id": "DXqTmgdE-dXi"
      },
      "outputs": [],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# Increase timeout for geolocator (can be kept or adjusted, but lookup is more robust)\n",
        "geolocator = Nominatim(user_agent=\"geo_loadshedding_app\", timeout=10)\n",
        "\n",
        "def get_province_coordinates(province_name):\n",
        "    \"\"\"Gets latitude and longitude for a given province name.\"\"\"\n",
        "    try:\n",
        "        # Geocode the province name in South Africa\n",
        "        location = geolocator.geocode(province_name + \", South Africa\")\n",
        "        if location:\n",
        "            return location.latitude, location.longitude\n",
        "        else:\n",
        "            print(f\"Could not geocode province: {province_name}\")\n",
        "            return np.nan, np.nan\n",
        "    except Exception as e:\n",
        "        print(f\"Error geocoding {province_name}: {e}\")\n",
        "        return np.nan, np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start of modified code ---\n",
        "\n",
        "print(\"Converting province names to geographical coordinates...\")\n",
        "\n",
        "# Get unique province names\n",
        "unique_provinces = loadshedding_data['province'].unique()\n",
        "\n",
        "# Create a dictionary to store coordinates for each province\n",
        "province_coordinates = {}\n",
        "\n",
        "print(\"Geocoding unique provinces...\")\n",
        "for province in unique_provinces:\n",
        "    lat, lon = get_province_coordinates(province)\n",
        "    province_coordinates[province] = {'latitude': lat, 'longitude': lon}\n",
        "    print(f\"Geocoded {province}: Lat={lat}, Lon={lon}\")\n",
        "\n",
        "    # Map the coordinates back to the DataFrame\n",
        "loadshedding_data['latitude'] = loadshedding_data['province'].map(lambda x: province_coordinates[x]['latitude'])\n",
        "loadshedding_data['longitude'] = loadshedding_data['province'].map(lambda x: province_coordinates[x]['longitude'])\n",
        "\n",
        "# --- End of modified code ---"
      ],
      "metadata": {
        "id": "FJ2DjgRUzdP8"
      },
      "id": "FJ2DjgRUzdP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows with the new columns\n",
        "print(\"\\nDataFrame with Latitude and Longitude:\")\n",
        "display(loadshedding_data.head())"
      ],
      "metadata": {
        "id": "GMneE4TdzdhC"
      },
      "id": "GMneE4TdzdhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a696c127"
      },
      "source": [
        "## Step 8: Acquire or generate loadshedding data with geographical information (e.g., location points, areas, or regions)."
      ],
      "id": "a696c127"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78d2ddff"
      },
      "source": [
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_geo_loadshedding_data(start_date='2023-01-01', end_date='2024-01-01', freq='H', num_locations=100):\n",
        "    \"\"\"\n",
        "    Generates synthetic load shedding data with simulated geographical locations within South Africa.\n",
        "\n",
        "    Args:\n",
        "        start_date (str): Start date for data generation.\n",
        "        end_date (str): End date for data generation.\n",
        "        freq (str): Frequency of observations (e.g., 'H' for hourly).\n",
        "        num_locations (int): Number of unique simulated locations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Generated loadshedding data with timestamp, stage, latitude, and longitude.\n",
        "    \"\"\"\n",
        "    print(\"üîß Generating synthetic load shedding data with geographical information...\")\n",
        "\n",
        "    dates = pd.date_range(start_date, end_date, freq=freq)\n",
        "    total_time_points = len(dates)\n",
        "\n",
        "    # Simulate locations within a simplified bounding box for South Africa\n",
        "    # South Africa approximate bounding box: Latitude (-35, -22), Longitude (17, 33)\n",
        "    min_lat, max_lat = -35, -22\n",
        "    min_lon, max_lon = 17, 33\n",
        "\n",
        "    np.random.seed(42) # for reproducibility\n",
        "\n",
        "    # Generate random locations\n",
        "    locations = pd.DataFrame({\n",
        "        'location_id': range(num_locations),\n",
        "        'latitude': np.random.uniform(min_lat, max_lat, num_locations),\n",
        "        'longitude': np.random.uniform(min_lon, max_lon, num_locations)\n",
        "    })\n",
        "\n",
        "    # Create a dataframe with all combinations of dates and locations\n",
        "    from itertools import product\n",
        "    data = pd.DataFrame(list(product(dates, locations['location_id'])), columns=['timestamp', 'location_id'])\n",
        "\n",
        "    # Merge with location coordinates\n",
        "    data = pd.merge(data, locations, on='location_id')\n",
        "\n",
        "    # Simulate load shedding stages (simplified logic for demonstration)\n",
        "    # Based on time of day and a general trend\n",
        "    data['hour'] = data['timestamp'].dt.hour\n",
        "    data['stage'] = 0 # Start with no loadshedding\n",
        "\n",
        "    # Increase stage during peak hours (e.g., 6-9 and 17-20)\n",
        "    peak_hours_mask = data['hour'].isin(range(6, 10)) | data['hour'].isin(range(17, 21))\n",
        "    data.loc[peak_hours_mask, 'stage'] = np.random.choice([0, 1, 2], size=peak_hours_mask.sum(), p=[0.4, 0.3, 0.3])\n",
        "\n",
        "    # Further increase stage randomly for some events to simulate variability\n",
        "    random_increase_mask = np.random.rand(len(data)) < 0.1 # 10% chance of random increase\n",
        "    data.loc[random_increase_mask, 'stage'] = np.minimum(data.loc[random_increase_mask, 'stage'] + np.random.randint(0, 3, size=random_increase_mask.sum()), 4)\n",
        "\n",
        "    # Ensure stage is integer\n",
        "    data['stage'] = data['stage'].astype(int)\n",
        "\n",
        "    # Select relevant columns\n",
        "    data = data[['timestamp', 'latitude', 'longitude', 'stage']]\n",
        "\n",
        "    print(f\"\\nüìä Generated Data Summary:\")\n",
        "    print(f\"   üìÖ Time range: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
        "    print(f\"   üìà Total observations: {len(data):,}\")\n",
        "    print(f\"   üìç Number of locations: {num_locations}\")\n",
        "    print(f\"   ‚ö° Active load shedding events: {(data['stage'] > 0).sum():,}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Generate synthetic data with geo-information\n",
        "geo_loadshedding_data = generate_geo_loadshedding_data()\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"\\nSample of generated geo-loadshedding data:\")\n",
        "display(geo_loadshedding_data.head())\n",
        "\n",
        "# Display info to check data types and non-nulls\n",
        "print(\"\\nDataFrame Info:\")\n",
        "geo_loadshedding_data.info()"
      ],
      "id": "78d2ddff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feb1885f"
      },
      "source": [
        "## Step 9: Data Cleaning\n",
        "\n",
        "- Identify and remove any duplicate data entries."
      ],
      "id": "feb1885f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8b3ad33"
      },
      "source": [
        "print(\"üîç Identifying and removing duplicate data entries...\")\n",
        "\n",
        "# Check for duplicate rows based on all columns\n",
        "initial_rows = len(geo_loadshedding_data)\n",
        "duplicate_rows = geo_loadshedding_data.duplicated().sum()\n",
        "\n",
        "if duplicate_rows > 0:\n",
        "    print(f\"‚ùó Found {duplicate_rows:,} duplicate rows.\")\n",
        "    # Remove duplicate rows\n",
        "    geo_loadshedding_data = geo_loadshedding_data.drop_duplicates()\n",
        "    print(f\"‚úÖ Removed duplicate rows. New number of rows: {len(geo_loadshedding_data):,}\")\n",
        "else:\n",
        "    print(\"‚úÖ No duplicate rows found.\")\n",
        "\n",
        "print(\"\\nDataFrame after duplicate removal:\")\n",
        "display(geo_loadshedding_data.head())\n",
        "print(f\"üìä Final dataset shape: {geo_loadshedding_data.shape}\")"
      ],
      "id": "a8b3ad33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b794fa78"
      },
      "source": [
        "## Step 10: Map Visualization\n",
        "\n",
        "- Create an interactive map of South Africa using a library called folium."
      ],
      "id": "b794fa78"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cd79cd77"
      },
      "source": [
        "import folium\n",
        "\n",
        "print(\"Creating a basic interactive map of South Africa...\")\n",
        "\n",
        "# Approximate coordinates for the center of South Africa\n",
        "south_africa_center = [-28.5, 24.5]\n",
        "\n",
        "# Create a Folium map centered on South Africa\n",
        "m = folium.Map(location=south_africa_center, zoom_start=5)\n",
        "\n",
        "print(\"‚úÖ Map created. Displaying the map:\")\n",
        "\n",
        "# Display the map\n",
        "m"
      ],
      "id": "cd79cd77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "169b38c4"
      },
      "source": [
        "## Step 11: Map Visualization\n",
        "\n",
        "- Superimpose loadshedding data visually onto the map."
      ],
      "id": "169b38c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fd573503"
      },
      "source": [
        "print(\"üìç Superimposing loadshedding data onto the map...\")\n",
        "\n",
        "# Define a color mapping for loadshedding stages\n",
        "stage_colors = {\n",
        "    0: 'green',   # No loadshedding\n",
        "    1: 'yellow',  # Stage 1\n",
        "    2: 'orange',  # Stage 2\n",
        "    3: 'red',     # Stage 3\n",
        "    4: 'darkred'  # Stage 4\n",
        "}\n",
        "\n",
        "# Add markers for a sample of the data to the map\n",
        "# Limiting to the first 1000 rows for performance\n",
        "sample_data = geo_loadshedding_data.sample(min(1000, len(geo_loadshedding_data)), random_state=42)\n",
        "\n",
        "for index, row in sample_data.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        radius=5,\n",
        "        color=stage_colors.get(row['stage'], 'gray'),\n",
        "        fill=True,\n",
        "        fill_color=stage_colors.get(row['stage'], 'gray'),\n",
        "        fill_opacity=0.7,\n",
        "        tooltip=f\"Stage: {row['stage']}<br>Timestamp: {row['timestamp']}\"\n",
        "    ).add_to(m)\n",
        "\n",
        "print(f\"‚úÖ Added {len(sample_data)} markers to the map.\")\n",
        "print(\"üí° Displaying the map with loadshedding data:\")\n",
        "\n",
        "# Display the map with markers\n",
        "m"
      ],
      "id": "fd573503",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b36a5f2"
      },
      "source": [
        "## Step 12: Add interactivity and refine the visualization\n",
        "\n"
      ],
      "id": "2b36a5f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4894b65d"
      },
      "source": [
        "print(\"‚ú® Adding interactivity and refining map visualization...\")\n",
        "\n",
        "# Create a FeatureGroup for each loadshedding stage\n",
        "stage_layers = {}\n",
        "for stage, color in stage_colors.items():\n",
        "    stage_layers[stage] = folium.FeatureGroup(name=f'Stage {stage}')\n",
        "\n",
        "# Add markers to their respective FeatureGroups\n",
        "for index, row in sample_data.iterrows():\n",
        "    stage = row['stage']\n",
        "    folium.CircleMarker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        radius=5,\n",
        "        color=stage_colors.get(stage, 'gray'),\n",
        "        fill=True,\n",
        "        fill_color=stage_colors.get(stage, 'gray'),\n",
        "        fill_opacity=0.7,\n",
        "        tooltip=f\"Stage: {stage}<br>Timestamp: {row['timestamp']}\"\n",
        "    ).add_to(stage_layers[stage])\n",
        "\n",
        "# Add FeatureGroups to the map\n",
        "for stage, layer in stage_layers.items():\n",
        "    layer.add_to(m)\n",
        "\n",
        "# Add a layer control to the map\n",
        "folium.LayerControl().add_to(m)\n",
        "\n",
        "print(\"‚úÖ Interactivity added. Displaying the enhanced map:\")\n",
        "print(\"üí° You can now toggle different loadshedding stages on/off using the layer control.\")\n",
        "\n",
        "# Display the enhanced map\n",
        "m"
      ],
      "id": "4894b65d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcac371f"
      },
      "source": [
        "## Step 13: Map Visualization (Heatmap)\n",
        "\n",
        "- Create a heatmap visualization of loadshedding data."
      ],
      "id": "fcac371f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "639d8aff"
      },
      "source": [
        "import folium.plugins\n",
        "\n",
        "print(\"üî• Creating a heatmap visualization of loadshedding data...\")\n",
        "\n",
        "# Ensure data is in the correct format for HeatMap (list of lists or numpy array)\n",
        "# [latitude, longitude, intensity] - using stage as intensity\n",
        "heatmap_data = geo_loadshedding_data[['latitude', 'longitude', 'stage']].values.tolist()\n",
        "\n",
        "# Create a Folium map centered on South Africa (reuse the existing map object 'm' or create a new one if needed)\n",
        "# For this example, we will reuse the existing map 'm' from previous steps.\n",
        "# If you run this cell independently, you might need to create a new map:\n",
        "# south_africa_center = [-28.5, 24.5]\n",
        "# m_heatmap = folium.Map(location=south_africa_center, zoom_start=5)\n",
        "\n",
        "# Add the heatmap layer to the map\n",
        "folium.plugins.HeatMap(heatmap_data).add_to(m)\n",
        "\n",
        "print(\"‚úÖ Heatmap layer added to the map.\")\n",
        "print(\"üí° Displaying the map with heatmap:\")\n",
        "\n",
        "# Display the map with the heatmap layer\n",
        "m"
      ],
      "id": "639d8aff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Key Takeaways\n",
        "- Loading and inspecting geospatial data in pandas\n",
        "- Using Folium for interactive base maps\n",
        "- Plotting scalable and clustered markers\n",
        "- (Optional) Visualizing density via heatmaps\n"
      ],
      "metadata": {
        "id": "x8VFdogseO1J"
      },
      "id": "x8VFdogseO1J"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}